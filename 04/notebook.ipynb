{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36ed41c-fbb9-4e99-a583-46e5898ac9e4",
   "metadata": {},
   "source": [
    "Follow along this tutorial: https://github.com/alexeygrigorev/rag-agents-workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f2c47-4552-4620-bc04-c484617aa597",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f94620-4db6-4154-915c-cce211e5b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29ed14f-af3a-4340-8f40-1c7c2cc207c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x7f741e7f1ee0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8749d06-f715-4ab5-ac29-9f35bee27352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65e06c-8fb5-4bf8-b24e-09661ed36d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Can I still join the course?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4be2c8-2f4e-4590-aef2-01220a82c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8aa058-c45e-42b4-997e-40c8aa384b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f8d40-a251-4fbf-b210-f602df8def75",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(question, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a86d4c6-02ed-404f-ba48-5b09ca16de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6300ca-29d9-420e-9c4d-d49fd5088239",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88130ac7-7f1a-44ea-9d93-a611131f2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e1607-5dfc-422d-af86-2179e3e4e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51608232-f6a9-4345-ba90-9f10054ad323",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag(\"How do I patch KDE under FreeBSD?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee43fa1-ce00-4598-9b21-2c8a17d65c09",
   "metadata": {},
   "source": [
    "## \"Agentic\" RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc51b91-5779-43b2-80c6-c7159afbb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "At the beginning the context is EMPTY.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "If CONTEXT is EMPTY, you can use our FAQ database.\n",
    "In this case, use the following output template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\"\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae701f4-342a-41fd-9ab6-46e3eceff7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Can I still join the course?'\n",
    "context = 'EMPTY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b066f-3f6a-4a7a-aeab-bda815dcacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d87856-489e-4a8b-8818-271d0da83188",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350ac019-50cf-48a2-b47b-05fa13cf6ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bd647-f385-4ab7-84e7-4bf231c99021",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a80494-09fd-4691-86ce-57dff7bbab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258864cf-0fff-46cc-9436-227b4c918b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e614a-42b3-4c81-9940-8eae90193e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658330f-cfd4-4400-9d73-dc149dc1672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(question)\n",
    "context = build_context(search_results)\n",
    "prompt = prompt_template.format(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71737247-862f-48f9-a59c-a5f1d5ef575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c7992-6dad-431e-8cec-3407d3a38f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ce883-726b-4039-a18b-d53a38ffcb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_rag_v1(question):\n",
    "    context = \"EMPTY\"\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(answer)\n",
    "\n",
    "    if answer['action'] == 'SEARCH':\n",
    "        print('need to perform search...')\n",
    "        search_results = search(question)\n",
    "        context = build_context(search_results)\n",
    "        \n",
    "        prompt = prompt_template.format(question=question, context=context)\n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7cb52-73a5-4f5c-b3ee-d3b93697e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentic_rag_v1('how do I join this course ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dae11b-1170-4477-9361-8ee52885a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentic_rag_v1('how patch KDE under FreeBSD?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbdbca-ebde-4dc4-8b4a-11d93232516f",
   "metadata": {},
   "source": [
    "## Agentic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c272d4-d5a8-4e3f-9484-c3a5ffa4db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for el in seq:\n",
    "        _id = el['_id']\n",
    "        if _id in seen:\n",
    "            continue\n",
    "        seen.add(_id)\n",
    "        result.append(el)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2fbf0-9800-4d24-a5e7-083482657720",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "\n",
    "The CONTEXT is build with the documents from our FAQ database.\n",
    "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
    "from FAQ to and add them to the context.\n",
    "PREVIOUS_ACTIONS contains the actions you already performed.\n",
    "\n",
    "At the beginning the CONTEXT is empty.\n",
    "\n",
    "You can perform the following actions:\n",
    "\n",
    "- Search in the FAQ database to get more data for the CONTEXT\n",
    "- Answer the question using the CONTEXT\n",
    "- Answer the question using your own knowledge\n",
    "\n",
    "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
    "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
    "\n",
    "Don't use search queries used at the previous iterations.\n",
    "\n",
    "Don't repeat previously performed actions.\n",
    "\n",
    "Don't perform more than {max_iterations} iterations for a given student question.\n",
    "The current iteration number: {iteration_number}. If we exceed the allowed number \n",
    "of iterations, give the best possible answer with the provided information.\n",
    "\n",
    "Output templates:\n",
    "\n",
    "If you want to perform search, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\",\n",
    "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER_CONTEXT\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<SEARCH_QUERIES>\n",
    "{search_queries}\n",
    "</SEARCH_QUERIES>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "<PREVIOUS_ACTIONS>\n",
    "{previous_actions}\n",
    "</PREVIOUS_ACTIONS>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e426c-8135-4d13-bfe9-b60e21346ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'how do I do well on module 1'\n",
    "max_iterations = 3\n",
    "iteration_number = 0\n",
    "search_queries = []\n",
    "search_results  = []\n",
    "previous_actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa40dfb-1fd0-4b2a-a0e3-8e15be4ae7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = build_context(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb2433-e98a-4df1-adaa-15a6e4ac41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aee99b-850f-4ced-82e0-07f365284798",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=max_iterations,\n",
    "    iteration_number=iteration_number\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d87ee-3262-4f49-9760-7ecb3c931c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c162430-5f56-40fd-9b8c-e95a31cc572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b28d5-2018-43c5-90c9-acb61aa9d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e6100-a9ff-4871-889f-aaafda644f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_actions.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc0689-b8b8-4eb4-8a9d-3431abc2d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e3965-8260-4394-a1f3-e8c8e9785e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = answer['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcb7c3-8a3b-4f90-9e7c-4aa596417fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ed6f7-bf4e-494f-9ff1-ffcae5d9b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kw in keywords:\n",
    "    search_queries.append(kw)\n",
    "    sr = search(kw)\n",
    "    search_results.extend(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edcdb00-5a54-4bbe-af72-e837c37d091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = dedup(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84cf11d-6673-474a-b3fc-efadae0f067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_number = 2\n",
    "\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=max_iterations,\n",
    "    iteration_number=iteration_number\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c3127-5954-4b19-83d1-795f0c0c1525",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ce542-e293-41fb-93d7-e2a9775b8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770a0aa-b057-4197-92f5-2f1f4d33dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b985cc-1b98-4522-aeaf-56e6f2beaab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what do I need to do to be successful at this llm course?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    print(f'ITERATION #{iteration}...')\n",
    "\n",
    "    context = build_context(search_results)\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        search_queries=\"\\n\".join(search_queries),\n",
    "        previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "        max_iterations=3,\n",
    "        iteration_number=iteration\n",
    "    )\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(json.dumps(answer, indent=2))\n",
    "\n",
    "    previous_actions.append(answer)\n",
    "\n",
    "    action = answer['action']\n",
    "    if action != 'SEARCH':\n",
    "        break\n",
    "\n",
    "    keywords = answer['keywords']\n",
    "    search_queries = list(set(search_queries) | set(keywords))\n",
    "    \n",
    "    for k in keywords:\n",
    "        res = search(k)\n",
    "        search_results.extend(res)\n",
    "\n",
    "    search_results = dedup(search_results)\n",
    "    \n",
    "    iteration = iteration + 1\n",
    "    if iteration >= 4:\n",
    "        break\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d41418-6290-4867-9347-0bd5f86a51dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3a7bc-ee48-4888-9652-7f9a09dfedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb5d73-0425-4f8b-98dd-113e88345aa6",
   "metadata": {},
   "source": [
    "## Function calling (\"tool use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6aab03e-f653-4e82-9362-ddfba53ce286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa30ef3a-99e6-4e50-94ef-9d76a7431bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de560ff4-7a38-4734-abe2-5149459eb873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_call(tool_call_response):\n",
    "    function_name = tool_call_response.name\n",
    "    arguments = json.loads(tool_call_response.arguments)\n",
    "\n",
    "    f = globals()[function_name]\n",
    "    result = f(**arguments)\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": tool_call_response.call_id,\n",
    "        \"output\": json.dumps(result, indent=2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea2ff680-e6f7-431a-a6f1-7fc6f47f184e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_wjhz6s3RI7gXtVR8SDSu1x9I', name='search', type='function_call', id='fc_68855f8c301881a2bfc63964e32361f60f4a361771e540a9', status='completed'),\n",
       " ResponseFunctionToolCall(arguments='{\"query\":\"study tips for module 1\"}', call_id='call_S6C1qhX3WqoTClxOI2Y2z4K9', name='search', type='function_call', id='fc_68855f8c620081a2b11aebf16e3357000f4a361771e540a9', status='completed')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I do well in module 1?\"\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "If you look up something in FAQ, convert the student question into multiple queries.\n",
    "\"\"\".strip()\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f2d0c70-621d-453f-a519-35a0fcb37557",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8776293-78b8-4180-93e0-5c399f8c6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for call in calls:\n",
    "    result = do_call(call)\n",
    "    chat_messages.append(call)\n",
    "    chat_messages.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b4ce91c-6287-4c7d-bfe9-82edbfc591c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseOutputMessage(id='msg_68855fb3170081a2aae7e6ff1032fc130f4a361771e540a9', content=[ResponseOutputText(annotations=[], text=\"To excel in Module 1, consider the following strategies:\\n\\n1. **Understand the Basics**: Ensure you have a solid grasp of foundational concepts related to Docker and Terraform, as these are crucial for this module.\\n\\n2. **Practice Coding**: Engage with hands-on coding exercises. For instance, experiment with SQLAlchemy commands and make note of common errors (like the `TypeError: 'module' object is not callable`).\\n\\n3. **Address Common Errors**:\\n   - If you encounter `ModuleNotFoundError: No module named 'psycopg2'`, make sure to install the necessary packages using:\\n     ```bash\\n     pip install psycopg2-binary\\n     ```\\n\\n4. **Explore Resources**: Utilize the course materials and documentation related to Docker and Terraform. This includes troubleshooting tips and best practices.\\n\\n5. **Collaborate with Peers**: Join study groups or discussion forums. Sharing challenges and solutions can enhance your understanding significantly.\\n\\n6. **Utilize Online Forums**: If you're stuck, consider browsing or posting on platforms like Stack Overflow. Many learners face similar issues and community support can resolve them quickly.\\n\\n7. **Keep Track of Dependencies**: Pay attention to package versions and dependencies as outdated versions may lead to errors.\\n\\nAdopting these strategies will help strengthen your skills in Module 1 and improve your overall performance in the course.\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "197f39e2-a611-4ccf-9720-746b6f9202ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message\n",
      "To excel in Module 1, consider the following strategies:\n",
      "\n",
      "1. **Understand the Basics**: Ensure you have a solid grasp of foundational concepts related to Docker and Terraform, as these are crucial for this module.\n",
      "\n",
      "2. **Practice Coding**: Engage with hands-on coding exercises. For instance, experiment with SQLAlchemy commands and make note of common errors (like the `TypeError: 'module' object is not callable`).\n",
      "\n",
      "3. **Address Common Errors**:\n",
      "   - If you encounter `ModuleNotFoundError: No module named 'psycopg2'`, make sure to install the necessary packages using:\n",
      "     ```bash\n",
      "     pip install psycopg2-binary\n",
      "     ```\n",
      "\n",
      "4. **Explore Resources**: Utilize the course materials and documentation related to Docker and Terraform. This includes troubleshooting tips and best practices.\n",
      "\n",
      "5. **Collaborate with Peers**: Join study groups or discussion forums. Sharing challenges and solutions can enhance your understanding significantly.\n",
      "\n",
      "6. **Utilize Online Forums**: If you're stuck, consider browsing or posting on platforms like Stack Overflow. Many learners face similar issues and community support can resolve them quickly.\n",
      "\n",
      "7. **Keep Track of Dependencies**: Pay attention to package versions and dependencies as outdated versions may lead to errors.\n",
      "\n",
      "Adopting these strategies will help strengthen your skills in Module 1 and improve your overall performance in the course.\n"
     ]
    }
   ],
   "source": [
    "for entry in response.output:\n",
    "    chat_messages.append(entry)\n",
    "    print(entry.type)\n",
    "\n",
    "    if entry.type == 'function_call':      \n",
    "        result = do_call(entry)\n",
    "        chat_messages.append(result)\n",
    "    elif entry.type == 'message':\n",
    "        print(entry.content[0].text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f1bf28a-6051-4007-99e7-de18ca996ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "When using FAQ, perform deep topic exploration: make one request to FAQ,\n",
    "and then based on the results, make more requests.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09415738-bfab-4a58-a0ee-99fa25c2f343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What is this course about ? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"course overview\"}', call_id='call_LfduO3NC8aFSgMzZUok6H5Ug', name='search', type='function_call', id='fc_68856062f6cc819d91c83ab984f425150ba22409d13868f8', status='completed')\n",
      "\n",
      "The course is focused on Data Engineering, specifically designed to guide students through various tools and practices in this field. It includes learning about alternative data stacks, where you'll explore both GCP (Google Cloud Platform) and local installations. You'll have the chance to work with different tools and frameworks relevant to data engineering, and you'll also engage in peer reviews for projects.\n",
      "\n",
      "Key details include:\n",
      "- The course will start on January 15, 2024, at 17:00.\n",
      "- After the course, you'll have access to all materials for self-paced learning.\n",
      "- Certificates are only awarded to participants who finish the course in real-time with a live cohort.\n",
      "\n",
      "If you have further questions about specific topics or tools covered, feel free to ask! \n",
      "\n",
      "What specific area of data engineering are you most interested in exploring during this course?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what assignments are given as part of this course ? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"course assignments\"}', call_id='call_Xu7BDhZ8TAplKIiE1gw15LMZ', name='search', type='function_call', id='fc_6885608112e8819d8226dbc22242dee20ba22409d13868f8', status='completed')\n",
      "\n",
      "The FAQ didn’t provide specific details about the assignments for this course. However, I can share that typically, in a Data Engineering course, assignments may include:\n",
      "\n",
      "1. **Hands-on Projects**: Implementing projects that involve data pipelines, using tools such as Airflow, GCP, or similar.\n",
      "2. **Homework Assignments**: Tasks that reinforce learning from lectures, often involving coding exercises or data manipulation tasks.\n",
      "3. **Capstone Project**: A final project that usually involves the application of the concepts and tools learned throughout the course, which will require peer review.\n",
      "\n",
      "Since the course emphasizes practical skills, you'll want to be prepared for a combination of theoretical knowledge and hands-on experiences.\n",
      "\n",
      "Would you like to know more about any specific tools or aspects of the assignments?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Does this course teach DBT ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"course DBT\"}', call_id='call_Re7cXKJnBawdT8dWvEcSB7lT', name='search', type='function_call', id='fc_688560a03d30819d8272450228e55a640ba22409d13868f8', status='completed')\n",
      "\n",
      "Yes, this course includes instruction on **dbt** (data build tool), particularly in the context of analytics engineering. You'll learn how to use dbt with BigQuery and gain hands-on experience with common tasks and error troubleshooting, such as:\n",
      "\n",
      "- Connecting dbt with BigQuery and resolving connection errors.\n",
      "- Handling data type issues when running dbt models.\n",
      "- Setting up continuous integration (CI) jobs in dbt Cloud.\n",
      "\n",
      "This will help you strengthen your skills in transforming data and preparing datasets for analysis.\n",
      "\n",
      "If you have any specific questions about dbt or its application in the course, feel free to ask! Is there a particular aspect of dbt you’re curious about?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What Database they use ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"database used in course\"}', call_id='call_Mesz7UHMmzOCe2BtGT9wECKq', name='search', type='function_call', id='fc_688560b705dc819dbd496d1b6beedc020ba22409d13868f8', status='completed')\n",
      "\n",
      "The course uses multiple databases as part of its educational framework. Specifically, you will work with:\n",
      "\n",
      "1. **PostgreSQL**: This is often utilized for various assignments and exercises, especially in a Docker environment.\n",
      "2. **BigQuery**: Used in conjunction with dbt for analytics engineering tasks.\n",
      "\n",
      "The flexibility of the course allows for the use of different databases and tools, providing a comprehensive understanding of data engineering.\n",
      "\n",
      "If there's a specific database feature or tool you want to know more about, let me know! Which database are you most interested in learning more about?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Bigquery ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"BigQuery in course\"}', call_id='call_Jt70PZBkNe4pXkcIxeVUSJtE', name='search', type='function_call', id='fc_688560d10084819da673018cdaa0e48f0ba22409d13868f8', status='completed')\n",
      "\n",
      "The course incorporates **Google BigQuery** as a significant part of the curriculum. Here’s what you can expect with BigQuery:\n",
      "\n",
      "1. **Analytics Engineering**: You'll use BigQuery in conjunction with dbt to build and transform datasets for analysis.\n",
      "2. **Hands-On Practice**: There will be practical assignments that allow you to interact with BigQuery, focusing on querying large datasets efficiently and effectively.\n",
      "3. **Error Handling**: The course also addresses common issues encountered when working with BigQuery, such as data type mismatches and connection errors.\n",
      "\n",
      "This integration provides you with valuable experience in working with one of the industry's leading cloud data warehouses.\n",
      "\n",
      "If you have specific questions about working with BigQuery or its features, let me know! What aspects of BigQuery are you most interested in exploring?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " stop\n"
     ]
    }
   ],
   "source": [
    "while True: # main Q&A loop\n",
    "    question = input() # How do I do my best for module 1?\n",
    "    if question == 'stop':\n",
    "        break\n",
    "\n",
    "    message = {\"role\": \"user\", \"content\": question}\n",
    "    chat_messages.append(message)\n",
    "\n",
    "    while True: # request-response loop - query API till get a message\n",
    "        response = client.responses.create(\n",
    "            model='gpt-4o-mini',\n",
    "            input=chat_messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "        has_messages = False\n",
    "        \n",
    "        for entry in response.output:\n",
    "            chat_messages.append(entry)\n",
    "        \n",
    "            if entry.type == 'function_call':      \n",
    "                print('function_call:', entry)\n",
    "                print()\n",
    "                result = do_call(entry)\n",
    "                chat_messages.append(result)\n",
    "            elif entry.type == 'message':\n",
    "                print(entry.content[0].text)\n",
    "                print()\n",
    "                has_messages = True\n",
    "\n",
    "        if has_messages:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72e654-8f21-46a9-ac7a-ef67f7bd9120",
   "metadata": {},
   "source": [
    "## Multiple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af08b204-f01d-4ecf-8232-69ad68c80b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-26 23:12:35--  https://raw.githubusercontent.com/alexeygrigorev/rag-agents-workshop/refs/heads/main/chat_assistant.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3495 (3.4K) [text/plain]\n",
      "Saving to: ‘chat_assistant.py.1’\n",
      "\n",
      "chat_assistant.py.1 100%[===================>]   3.41K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-07-26 23:12:35 (49.5 MB/s) - ‘chat_assistant.py.1’ saved [3495/3495]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/alexeygrigorev/rag-agents-workshop/refs/heads/main/chat_assistant.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83cbac21-c207-4bbd-853e-e5c9dc5329da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry(question, answer):\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text': answer,\n",
    "        'section': 'user added',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15ecaa5c-f791-44c1-b09a-86e55caf9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_entry_description = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"add_entry\",\n",
    "    \"description\": \"Add an entry to the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The question to be added to the FAQ database\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The answer to the question\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76ca7181-427c-49e2-aa09-e486b13760f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chat_assistant\n",
    "\n",
    "tools = chat_assistant.Tools()\n",
    "tools.add_tool(search, search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0366224-695e-4295-972d-4e6857fdb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.add_tool(add_entry, add_entry_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e8cdea1-6e33-4fb8-a2ff-d8d59637d28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'Search query text to look up in the course FAQ.'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add an entry to the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'The question to be added to the FAQ database'},\n",
       "    'answer': {'type': 'string', 'description': 'The answer to the question'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78144b8f-0b15-466a-8eca-46ad3d92ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_interface = chat_assistant.ChatInterface()\n",
    "\n",
    "chat = chat_assistant.ChatAssistant(\n",
    "    tools=tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63323c5f-9793-4c7a-be3f-7eb26e15e411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: What is this course About ? \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"What is this course About\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"What is this course About\"}', call_id='call_Zj86Os8rAW4obuldRICQrduz', name='search', type='function_call', id='fc_6885615223a881929e7c419bae74c9b304f0d03574182014', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  \\u201cOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon\\u2019t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Course - When will the course start?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 0\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Course - Can I follow the course after it finishes?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 7\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"No, you can only get a certificate if you finish the course with a \\u201clive\\u201d cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Certificate - Can I follow the course in a self-paced mode and get a certificate?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 11\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\\nShould you consider it instead of the one tool you use? That we can\\u2019t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Is it possible to use tool \\u201cX\\u201d instead of the one tool you use in the course?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 33\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors\\u2019 code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"How do I use Git / GitHub for this course?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 41\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>The course, \"Data Engineering Zoomcamp,\" focuses on providing an immersive experience in data engineering. It covers various tools and techniques essential for data engineering, including both Google Cloud Platform (GCP) products and local installations. Students can choose between different data stacks and explore alternatives based on their specific needs.</p>\n",
       "<p>You'll learn about data pipelines, project design, and tools like Airflow, Prefect, AWS, Snowflake, and more, along with hands-on projects that culminate in a capstone project.</p>\n",
       "<p>Would you like more details about the tools that will be covered or the structure of the course?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Do I need to use AWS or GCP ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"Is it possible to use tool X instead ...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"Is it possible to use tool X instead of GCP\"}', call_id='call_hZk44fRrnxpkc0M0Fz5e4oLX', name='search', type='function_call', id='fc_688561602ffc8192b96ec297376f8cc704f0d03574182014', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\\nShould you consider it instead of the one tool you use? That we can\\u2019t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Is it possible to use tool \\u201cX\\u201d instead of the one tool you use in the course?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 33\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won\\u2019t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\\nFor everything in the course, there\\u2019s a local alternative. You could even do the whole course locally.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Environment - The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 27\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Yes, you can use any tool you want for your project.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Can I use Airflow instead for my final project?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 32\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Either use conda or pip for managing venv, using both of them together will cause incompatibility.\\nIf you\\u2019re using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\\nconda install -c conda-forge psycopg2\\nIf pip, do the normal install\\npip install psycopg2\",\n",
       "    \"section\": \"Project\",\n",
       "    \"question\": \"psycopg2 complains of incompatible environment e.g x86 instead of amd\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 407\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"GCP VM - Is it necessary to use a GCP VM? When is it useful?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 135\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>No, you are not required to use AWS or GCP for the course. The course covers two alternative data stacks: one utilizing GCP and another with a local installation. Almost everything can be run locally, except for some specific tools like BigQuery.</p>\n",
       "<p>You are free to use any tool of your choice for your projects, although the course materials may primarily focus on the specified stacks. If you decide to use a different stack, keep in mind that you might need to explain your tool choices during the peer review of your capstone project.</p>\n",
       "<p>Are you considering using a specific tool or platform for your projects?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "chat.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d569e-ef87-4afe-839c-f873498d54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265a810-7720-4f19-8897-c34fbba4bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.docs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d8a4e91-a82e-44fe-983e-3c5a07d18968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic-ai\n",
      "  Downloading pydantic_ai-0.4.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic-ai-slim==0.4.7 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading pydantic_ai_slim-0.4.7-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting griffe>=1.3.2 (from pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading griffe-1.8.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.28.1)\n",
      "Collecting opentelemetry-api>=1.28.0 (from pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pydantic-graph==0.4.7 (from pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading pydantic_graph-0.4.7-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pydantic>=2.10 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (2.10.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.4.1)\n",
      "Collecting ag-ui-protocol>=0.1.8 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading ag_ui_protocol-0.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: starlette>=0.45.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.45.3)\n",
      "Collecting anthropic>=0.52.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading anthropic-0.59.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: boto3>=1.37.24 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.38.27)\n",
      "Collecting argcomplete>=3.5.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (3.0.51)\n",
      "Requirement already satisfied: rich>=13 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (14.0.0)\n",
      "Collecting cohere>=5.16.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading cohere-5.16.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pydantic-evals==0.4.7 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading pydantic_evals-0.4.7-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting google-genai>=1.24.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading google_genai-1.27.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting groq>=0.19.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading groq-0.30.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting huggingface-hub>=0.33.5 (from huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading huggingface_hub-0.34.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting mcp>=1.10.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading mcp-1.12.2-py3-none-any.whl.metadata (60 kB)\n",
      "Collecting mistralai>=1.9.2 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading mistralai-1.9.3-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting openai>=1.92.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading openai-1.97.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting google-auth>=2.36.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (2.32.3)\n",
      "Requirement already satisfied: anyio>=0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-evals==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (4.9.0)\n",
      "Collecting logfire-api>=1.2.0 (from pydantic-evals==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading logfire_api-4.0.0-py3-none-any.whl.metadata (971 bytes)\n",
      "Requirement already satisfied: pyyaml>=6.0.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-evals==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (6.0.2)\n",
      "Collecting pydantic>=2.10 (from pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.7.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.10->pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (4.12.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio>=0->pydantic-evals==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.16.0)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.27 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.38.27)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/codespace/.local/lib/python3.12/site-packages (from botocore<1.39.0,>=1.38.27->boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/codespace/.local/lib/python3.12/site-packages (from botocore<1.39.0,>=1.38.27->boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.27->boto3>=1.37.24->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.17.0)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere>=5.16.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading fastavro-1.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere>=5.16.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere>=5.16.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.21.1)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere>=5.16.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (3.4.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (24.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.1.4)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting tenacity<9.0.0,>=8.2.3 (from google-genai>=1.24.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai>=1.24.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: colorama>=0.4 in /home/codespace/.local/lib/python3.12/site-packages (from griffe>=1.3.2->pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.4.6)\n",
      "Requirement already satisfied: aiohttp in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (3.12.13)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/codespace/.local/lib/python3.12/site-packages (from mcp>=1.10.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (4.24.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp>=1.10.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from mcp>=1.10.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.0.20)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp>=1.10.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading sse_starlette-3.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting uvicorn>=0.23.1 (from mcp>=1.10.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.10.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.10.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.10.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.10.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.25.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==0.4.7->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (3.23.0)\n",
      "Requirement already satisfied: wcwidth in /home/codespace/.local/lib/python3.12/site-packages (from prompt-toolkit>=3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.2.13)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp>=1.10.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.1.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from uvicorn>=0.23.1->mcp>=1.10.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (8.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,mcp,mistral,openai,vertexai]==0.4.7->pydantic-ai) (1.20.1)\n",
      "Downloading pydantic_ai-0.4.7-py3-none-any.whl (10 kB)\n",
      "Downloading pydantic_ai_slim-0.4.7-py3-none-any.whl (251 kB)\n",
      "Downloading pydantic_evals-0.4.7-py3-none-any.whl (52 kB)\n",
      "Downloading pydantic_graph-0.4.7-py3-none-any.whl (27 kB)\n",
      "Downloading ag_ui_protocol-0.1.8-py3-none-any.whl (7.1 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Downloading anthropic-0.59.0-py3-none-any.whl (293 kB)\n",
      "Downloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
      "Downloading cohere-5.16.1-py3-none-any.whl (291 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading fastavro-1.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.1-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading google_genai-1.27.0-py3-none-any.whl (218 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading griffe-1.8.0-py3-none-any.whl (132 kB)\n",
      "Downloading groq-0.30.0-py3-none-any.whl (131 kB)\n",
      "Downloading logfire_api-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mcp-1.12.2-py3-none-any.whl (158 kB)\n",
      "Downloading mistralai-1.9.3-py3-none-any.whl (426 kB)\n",
      "Downloading openai-1.97.1-py3-none-any.whl (764 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.4/764.4 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading sse_starlette-3.0.0-py3-none-any.whl (11 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Installing collected packages: websockets, uvicorn, types-requests, tenacity, pydantic-core, pyasn1, logfire-api, httpx-sse, griffe, fastavro, eval-type-backport, cachetools, argcomplete, sse-starlette, rsa, pydantic, pyasn1-modules, opentelemetry-api, huggingface-hub, pydantic-graph, openai, mistralai, groq, google-auth, anthropic, ag-ui-protocol, pydantic-ai-slim, mcp, google-genai, cohere, pydantic-evals, pydantic-ai\n",
      "\u001b[2K  Attempting uninstall: tenacity;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/32\u001b[0m [uvicorn]\n",
      "\u001b[2K    Found existing installation: tenacity 9.1.27m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/32\u001b[0m [uvicorn]\n",
      "\u001b[2K    Uninstalling tenacity-9.1.2:m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/32\u001b[0m [uvicorn]\n",
      "\u001b[2K      Successfully uninstalled tenacity-9.1.2237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/32\u001b[0m [uvicorn]\n",
      "\u001b[2K  Attempting uninstall: pydantic-corem\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/32\u001b[0m [uvicorn]\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.27.2━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/32\u001b[0m [uvicorn]\n",
      "\u001b[2K    Uninstalling pydantic_core-2.27.2:\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/32\u001b[0m [uvicorn]\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.27.2━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/32\u001b[0m [uvicorn]\n",
      "\u001b[2K  Attempting uninstall: pydantic━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/32\u001b[0m [eval-type-backport]\n",
      "\u001b[2K    Found existing installation: pydantic 2.10.514m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/32\u001b[0m [eval-type-backport]\n",
      "\u001b[2K    Uninstalling pydantic-2.10.5:\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/32\u001b[0m [eval-type-backport]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.10.5;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/32\u001b[0m [eval-type-backport]\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [pyasn1-modules]]kport]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.33.00m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.33.0:m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.33.0;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/32\u001b[0m [huggingface-hub]\n",
      "\u001b[2K  Attempting uninstall: openai━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/32\u001b[0m [pydantic-graph]]\n",
      "\u001b[2K    Found existing installation: openai 1.87.038;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/32\u001b[0m [pydantic-graph]\n",
      "\u001b[2K    Uninstalling openai-1.87.0:━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/32\u001b[0m [pydantic-graph]\n",
      "\u001b[2K      Successfully uninstalled openai-1.87.0\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/32\u001b[0m [pydantic-graph]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/32\u001b[0m [pydantic-ai]7m━━\u001b[0m \u001b[32m30/32\u001b[0m [pydantic-evals]m]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cognee 0.2.0 requires pydantic==2.10.5, but you have pydantic 2.11.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed ag-ui-protocol-0.1.8 anthropic-0.59.0 argcomplete-3.6.2 cachetools-5.5.2 cohere-5.16.1 eval-type-backport-0.2.2 fastavro-1.11.1 google-auth-2.40.3 google-genai-1.27.0 griffe-1.8.0 groq-0.30.0 httpx-sse-0.4.0 huggingface-hub-0.34.1 logfire-api-4.0.0 mcp-1.12.2 mistralai-1.9.3 openai-1.97.1 opentelemetry-api-1.35.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.7 pydantic-ai-0.4.7 pydantic-ai-slim-0.4.7 pydantic-core-2.33.2 pydantic-evals-0.4.7 pydantic-graph-0.4.7 rsa-4.9.1 sse-starlette-3.0.0 tenacity-8.5.0 types-requests-2.32.4.20250611 uvicorn-0.35.0 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydantic-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "578268cd-a7b0-414a-b99a-89706df43c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, RunContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5739bdfc-9d80-40f8-a469-61a60d8db211",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_agent = Agent(  \n",
    "    'openai:gpt-4o-mini',\n",
    "    system_prompt=developer_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbf6801f-0f7d-4fee-acc3-da1704b8c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "@chat_agent.tool\n",
    "def search_tool(ctx: RunContext, query: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Search the FAQ for relevant entries matching the query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        The search query string provided by the user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of search results (up to 5), each containing relevance information \n",
    "        and associated output IDs.\n",
    "    \"\"\"\n",
    "    print(f\"search('{query}')\")\n",
    "    return search(query)\n",
    "\n",
    "\n",
    "@chat_agent.tool\n",
    "def add_entry_tool(ctx: RunContext, question: str, answer: str) -> None:\n",
    "    \"\"\"\n",
    "    Add a new question-answer entry to FAQ.\n",
    "\n",
    "    This function creates a document with the given question and answer, \n",
    "    tagging it as user-added content.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "        The question text to be added to the index.\n",
    "\n",
    "    answer : str\n",
    "        The answer or explanation corresponding to the question.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    return add_entry(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be5548c8-d93b-49d6-9b84-f6a77ae5ac56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search('course enrollment')\n",
      "Yes, you can still join the course! However, it's recommended to register before the official start date, which is on January 15, 2024, at 17:00. If you want to join, make sure to follow the registration link provided by the course.\n",
      "\n",
      "Would you like the link to register or need help with anything else?\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"I just discovered the course. Can I join now?\"\n",
    "agent_run = await chat_agent.run(user_prompt)\n",
    "print(agent_run.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4605e-ed59-40ee-8355-b88307325032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
